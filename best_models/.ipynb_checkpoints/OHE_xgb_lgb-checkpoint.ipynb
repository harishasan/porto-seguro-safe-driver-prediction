{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoded XGB and LGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the results in this Note book are made using sample porto seguero data, to produce best results download the data from competition <a herf='https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data'> page</a> on kaggle, and place in porto_data directory, and also bin the data using binning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#import required modules\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from numba import jit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier\n",
    "\n",
    "\n",
    "# to supress printing of exponential notation in pandas\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../porto_data/train.csv'\n",
    "TEST_PATH = '../porto_data/test.csv'\n",
    "# SUBMIT_NAME = 'te_llb_18_LGB_1.csv'train.replace(np.nan, -1, inplace=True)\n",
    "test.replace(np.nan, -1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    \"\"\"Gini Evaluation metric\n",
    "\n",
    "    Score Gini for give True target and predicted target values\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        y_true {np.array} -- True target values\n",
    "        y_prob {np.array} -- Predicted target values\n",
    "\n",
    "    Returns:\n",
    "        gini {float} -- calculated gini sccore\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "\n",
    "def auc_to_gini(score):\n",
    "    \"\"\"Converts AUC to Gini\n",
    "       \n",
    "    Arguments:\n",
    "        score {float} -- AUC score\n",
    "    \n",
    "    Returns:\n",
    "        [float] -- gini score\n",
    "    \"\"\"\n",
    "    \n",
    "    gini = (2 * score) - 1\n",
    "    return gini\n",
    "\n",
    "\n",
    "def bold(text_to_bold):\n",
    "    \"\"\"Bolds the given string\n",
    "       \n",
    "    Arguments:\n",
    "        text_to_bold {string} -- string to bold\n",
    "\n",
    "    Returns:\n",
    "        [string]: Bold string\n",
    "    \"\"\"\n",
    "    \n",
    "    bold = \"\\033[1m\"\n",
    "    reset = \"\\033[0;0m\"\n",
    "    bold_text = bold + text_to_bold + reset \n",
    "    \n",
    "    return bold_text\n",
    "\n",
    "#__author__ = harishasan\n",
    "def timer(start_time=None):\n",
    "    \"\"\"Prints time\n",
    "    \n",
    "    Initiate a time object, and prints total time consumed when again initialized object is passed as argument\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        start_time {[object]} -- initialized time object (default: {None})\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "       \n",
    "\n",
    "    \n",
    "def calculate_avg(scores):\n",
    "    \"\"\"calculates average of given allay or list\n",
    "    \n",
    "    Arguments:\n",
    "        scores {[np.array]} -- array of scores\n",
    "    \"\"\"\n",
    "    avg = sum(scores)/ float(len(scores))\n",
    "    return avg\n",
    "\n",
    "def stacker(stacker_model, s_train, target, s_test):\n",
    "    \"\"\"trains stacker model on base models predictions and test on base model test predictions dataset\n",
    "       \n",
    "    Arguments:\n",
    "        stacker_model {[type]} -- model to traina s stacker\n",
    "        s_train {[type]} -- stacker train set\n",
    "        target {[type]} -- target(y) for model\n",
    "        s_test {[type]} -- stacker test set\n",
    "    \n",
    "    Returns:\n",
    "        [pd.DataFrame] -- predictions dataframem\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = timer(None)\n",
    "    stacker_model.fit(s_train, target)\n",
    "    preds = stacker_model.predict_proba(s_test)[:, 1]\n",
    "    timer(start_time)\n",
    "    preds_df = pd.DataFrame(data=preds, columns=['target']) \n",
    "    \n",
    "    return preds_df\n",
    "\n",
    "\n",
    "\n",
    "def get_base_model_results(model, X, y, test, model_name='base_model', y_test=None, n_splits=5, random_state=0, save_fold_results=True, fold_results_path='base_models_results/'):\n",
    "    \"\"\"for given model produces OOF and test predictions\n",
    "    \n",
    "    For the given Number of Folds, trains the model on train set, predicts on Out OF FOld set and test set, and repeat for given number of Folds.\n",
    "    \n",
    "    Arguments:\n",
    "        model {Object} -- Model i.e XGB, LGB...\n",
    "        X {nd.array} -- Train set\n",
    "        y {np.array} -- target values of train set\n",
    "        test {nd.array} -- test set\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        y_test {np.array} -- test set target values (default: {None})\n",
    "        n_splits {int} -- Number of folds for Cross Validation split (default: {5})\n",
    "        random_state {int} -- random seed to use in making folds for Cross Validation (default: {0})\n",
    "        save_fold_results {bool} -- save out of fold predections and test or Not (default: True)\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        tuple -- tuple of Dataframe of Out Of Fold and test set Predictions\n",
    "    \"\"\"\n",
    "    #TODO: refactor this function in to small reusable chunks.\n",
    "    \n",
    "    print bold(\"STARTING ITERATION FOR\") + \" \" + bold(model_name)\n",
    "    \n",
    "    train_preds_auc = []\n",
    "    train_preds_gini = []\n",
    "    \n",
    "    holdout_preds_auc = []\n",
    "    holdout_preds_gini = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    test = np.array(test)\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state).split(X, y))\n",
    "    \n",
    "    stacker_train = np.zeros(X.shape[0])\n",
    "    stacker_foldtest = np.zeros((test.shape[0], n_splits))\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(folds):\n",
    "        print \"Fold {}\".format(str(i))\n",
    "        X_train = X[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        X_holdout = X[test_idx]\n",
    "        y_holdout = y[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        preds_train = model.predict_proba(X_train)[:, 1] \n",
    "        preds = model.predict_proba(X_holdout)[:, 1]\n",
    "        \n",
    "        train_gini = eval_gini(y_train, preds_train)\n",
    "        \n",
    "        holdout_gini = eval_gini(y_holdout, preds)\n",
    "        \n",
    "        \n",
    "        print \"Training gini = {}    Holdout gini = {}\\n\".format(str(train_gini), str(holdout_gini))\n",
    "        \n",
    "\n",
    "#         print \"making stacker train\"\n",
    "        stacker_train[test_idx] = preds\n",
    "        \n",
    "        stacker_foldtest[:, i] = model.predict_proba(test)[:, 1]\n",
    "        \n",
    "        #append fold results to list\n",
    "        train_preds_gini.append(train_gini)\n",
    "        \n",
    "        holdout_preds_gini.append(holdout_gini)\n",
    "    \n",
    "    \n",
    "    foldtest_mean = stacker_foldtest.mean(axis=1)\n",
    "    stacker_test = foldtest_mean\n",
    "    print \"\\nAverage gini of training = {}    Average gini of holdouts = {} \\n\".format(str(calculate_avg(train_preds_gini)), str(calculate_avg(holdout_preds_gini)))\n",
    "    \n",
    "    print \"Folds variance for train {:.3f}    Folds variance for holdouts = {:.3f}\\n\".format(np.std(train_preds_gini), np.std(holdout_preds_gini))\n",
    "    if y_test is not None:\n",
    "        print bold(\"Test GINI = \") + \" {:.5f}\".format(auc_to_gini(roc_auc_score(y_test, stacker_test)))\n",
    "    print \"\\nTraining time in Minutes {} \\n \\n\".format(str((time.time() - start_time)/60))   \n",
    "\n",
    "    oof_preds = pd.DataFrame(data=stacker_train, columns=['{}train'.format(model_name)])\n",
    "    test_preds = pd.DataFrame(data=stacker_test, columns=['{}test'.format(model_name)])\n",
    "    \n",
    "    if save_fold_results:\n",
    "        oof_preds.to_csv(fold_results_path + 'oof_{}.csv'.format(model_name), index=False)\n",
    "        test_preds.to_csv(fold_results_path + 'test_{}.csv'.format(model_name), index=False)\n",
    "    \n",
    "    return (oof_preds, test_preds)\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode(data, cols=None, drop_missing_cols=False):\n",
    "    \"\"\"\n",
    "   creates new dataframe with dummy(OHE) columns appended\n",
    "   Args:\n",
    "       cols: list of column names to one_hot encode\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    cat_cols = cols if cols is not None else [col for col in data.columns.values if 'cat' in col]\n",
    "    print cat_cols\n",
    "    cat_df = data[cat_cols]\n",
    "    ohe = pd.get_dummies(cat_df, columns=cat_cols)\n",
    "    \n",
    "    df = data.drop(cat_cols, axis=1)\n",
    "\n",
    "    df = pd.concat([df, ohe], axis=1)\n",
    "    if drop_missing_cols:\n",
    "        neg_cols = [col for col in df.columns.values if '-1' in col]\n",
    "    #     print neg_cols\n",
    "        df.drop(neg_cols, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_all_excepet(data, cols_list=None, dropid=True):\n",
    "       \n",
    "    # That is because in python setting a variable actually sets a reference to the variable. \n",
    "    # Almost every person learning python encounters this at some point. The solution is simply to copy the list:\n",
    "    cols_list = None if cols_list is None else cols_list[:]\n",
    "    if dropid:\n",
    "        cols_list.append('target')\n",
    "    else:\n",
    "        cols_list.append('target')\n",
    "        cols_list.append('id')\n",
    "    cols_to_drop = [col for col in data.columns.values if col not in cols_list]\n",
    "#     print cols_to_drop\n",
    "    data.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importatn features\n",
    "# from olivier, include ps_car_11 for better results\n",
    "train_features = [\n",
    "\"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "    'ps_car_11_cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.replace(np.nan, -1, inplace=True)\n",
    "test.replace(np.nan, -1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_test = test['id'].values\n",
    "id_train = train['id'].values\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_all_excepet(train, train_features)\n",
    "drop_all_excepet(test, train_features, dropid=False)\n",
    "\n",
    "#seperate target and test id\n",
    "target = train['target']\n",
    "train.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "test_id = test['id']\n",
    "test.drop(['id'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_11_cat']\n",
      "['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_11_cat']\n",
      "(119043, 192)\n",
      "(250, 155)\n"
     ]
    }
   ],
   "source": [
    "train_oh = one_hot_encode(train, drop_missing_cols=True)\n",
    "test_oh = one_hot_encode(test, drop_missing_cols=True)\n",
    "print train_oh.shape\n",
    "print test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define models\n",
    "#xgb\n",
    "xgb_model = XGBClassifier(    \n",
    "                        max_depth=6,learning_rate=0.07, n_estimators=450,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        gamma=10,scale_pos_weight=1.6,                   \n",
    "    nthread=4, min_child_weight=9, subsample=.8,colsample_bytree=.8, reg_lambda=1.4, \n",
    "    reg_alpha=10 \n",
    "                     )\n",
    "\n",
    "#lgb\n",
    "lgb_params = {'max_depth':5,'num_leaves':30,'learning_rate':0.05,'colsample_bytree':0.8,'max_bin':10, 'subsample':0.8, \\\n",
    "              'n_estimators':450,'subsample_freq':6, 'objective':'binary', 'num_threads':3, 'min_child_samples':600}\n",
    "lgb_model = LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_strain, xgb_stest = get_base_model_results(xgb_model, train_oh, y, test_oh, model_name='oh_xgb', save_fold_results=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_strain, lgb_stest = get_base_model_results(lgb_model, train_oh, y, test_oh, model_name='oh_lgb', save_fold_results=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save oof and test predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
