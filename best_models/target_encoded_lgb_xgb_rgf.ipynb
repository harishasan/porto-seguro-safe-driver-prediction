{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Target Encoded LGB , XGB and RGF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the results in this Note book are made using sample porto seguero data, to produce best results download the data from competition <a herf='https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data'> page</a> on kaggle, and place in porto_data directory, and also bin the data using binning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "#import required modules\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from numba import jit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier\n",
    "\n",
    "\n",
    "# to supress printing of exponential notation in pandas\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../porto_data/bin_train.csv'\n",
    "TEST_PATH = '../porto_data/bin_test.csv'\n",
    "# SUBMIT_NAME = 'te_llb_18_LGB_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    \"\"\"Gini Evaluation metric\n",
    "\n",
    "    Score Gini for give True target and predicted target values\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        y_true {np.array} -- True target values\n",
    "        y_prob {np.array} -- Predicted target values\n",
    "\n",
    "    Returns:\n",
    "        gini {float} -- calculated gini sccore\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "\n",
    "def auc_to_gini(score):\n",
    "    \"\"\"Converts AUC to Gini\n",
    "       \n",
    "    Arguments:\n",
    "        score {float} -- AUC score\n",
    "    \n",
    "    Returns:\n",
    "        [float] -- gini score\n",
    "    \"\"\"\n",
    "    \n",
    "    gini = (2 * score) - 1\n",
    "    return gini\n",
    "\n",
    "\n",
    "def bold(text_to_bold):\n",
    "    \"\"\"Bolds the given string\n",
    "       \n",
    "    Arguments:\n",
    "        text_to_bold {string} -- string to bold\n",
    "\n",
    "    Returns:\n",
    "        [string]: Bold string\n",
    "    \"\"\"\n",
    "    \n",
    "    bold = \"\\033[1m\"\n",
    "    reset = \"\\033[0;0m\"\n",
    "    bold_text = bold + text_to_bold + reset \n",
    "    \n",
    "    return bold_text\n",
    "\n",
    "#__author__ = harishasan\n",
    "def timer(start_time=None):\n",
    "    \"\"\"Prints time\n",
    "    \n",
    "    Initiate a time object, and prints total time consumed when again initialized object is passed as argument\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        start_time {[object]} -- initialized time object (default: {None})\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "       \n",
    "\n",
    "    \n",
    "def calculate_avg(scores):\n",
    "    \"\"\"calculates average of given allay or list\n",
    "    \n",
    "    Arguments:\n",
    "        scores {[np.array]} -- array of scores\n",
    "    \"\"\"\n",
    "    avg = sum(scores)/ float(len(scores))\n",
    "    return avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from olivier\n",
    "#https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_te_base_results(model, X, y, test_df, model_name, f_cats, fillna=False, random_state=0):\n",
    "    \"\"\"get target encoded OOF and test predections for the stacker,target encodes training and holdout seperately in each fold\n",
    "    to reduce bias\n",
    "     \n",
    "    Arguments:\n",
    "        model {Object} -- Model\n",
    "        X {nd.array} -- Train set\n",
    "        y {np.array} -- target values\n",
    "        test_df {nd.array} -- Test Set\n",
    "        model_name {string} -- naem of the model\n",
    "        f_cats {list} -- list of category columns to encode\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        fillna {bool} -- To fill NaN with mean or assign True (default: {False})\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print bold(\"STARTING ITERATION FOR\") + \" \" + bold(model_name)\n",
    "    start_time = timer(None)\n",
    "    y_valid_pred = 0*y\n",
    "    y_test_pred = 0\n",
    "\n",
    "    # Set up folds\n",
    "    K = 5\n",
    "    kf = StratifiedKFold(n_splits = K, random_state = random_state, shuffle = True)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    train_preds = []\n",
    "    eval_preds = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "\n",
    "        # Create data for this fold\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "        X_test = test_df.copy()\n",
    "        print \"Fold {}\".format(str(i))\n",
    "\n",
    "        # Enocode data\n",
    "        for f in f_cats:\n",
    "            X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                            trn_series=X_train[f],\n",
    "                                                            val_series=X_valid[f],\n",
    "                                                            tst_series=X_test[f],\n",
    "                                                            target=y_train,\n",
    "                                                            min_samples_leaf=200,\n",
    "                                                            smoothing=10,\n",
    "                                                            noise_level=0\n",
    "                                                            )\n",
    "            \n",
    "        #fill NaN for Rgf\n",
    "        if fillna is True:\n",
    "            X_train.fillna(X_train.mean(), inplace=True)\n",
    "            X_valid.fillna(X_valid.mean(), inplace=True)\n",
    "            X_test.fillna(X_test.mean(), inplace=True)\n",
    "       \n",
    "       \n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "\n",
    "\n",
    "        preds_train = fit_model.predict_proba(X_train)[:,1]\n",
    "        # Generate validation predictions for this fold\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "\n",
    "        #calculate train and valid gini\n",
    "        train_gini = eval_gini(y_train, preds_train)\n",
    "        val_gini = eval_gini(y_valid, pred)\n",
    "\n",
    "        print \"Train Gini = {}    holdout Gini = {}\".format(str(train_gini), str(val_gini)) \n",
    "        y_valid_pred.iloc[test_index] = pred\n",
    "\n",
    "        # Accumulate test set predictions\n",
    "        y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "        train_preds.append(train_gini)\n",
    "        eval_preds.append(val_gini)\n",
    "\n",
    "        del X_test, X_train, X_valid, y_train\n",
    "\n",
    "    y_test_pred /= K  # Average test set predictions\n",
    "    print \"\\nGini for full holdout set:\", eval_gini(y, y_valid_pred)\n",
    "    \n",
    "    \n",
    "    print \"Avg Train gini = {}    Avg holdout gini = {}\".format(str(calculate_avg(train_preds)), str(calculate_avg(eval_preds)))\n",
    "    print \"Fold Variance for train = {:.3f}    Fold Variance for holdouts = {:.3f}\".format(np.std(train_preds),np.std(eval_preds))\n",
    "    \n",
    "    \n",
    "    timer(start_time)    \n",
    "\n",
    "    oof_preds = pd.DataFrame(data=y_valid_pred, columns=['{}_oof'.format(model_name)])\n",
    "    test_preds = pd.DataFrame(data=y_test_pred, columns=['{}_test'.format(model_name)])\n",
    "    \n",
    "    return oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importatn features\n",
    "# from olivier: https://www.kaggle.com/ogrellier/noise-analysis-of-porto-seguro-s-features\n",
    "train_features_bestxgb = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs_bestxgb = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define base model\n",
    "lgb_params = {'learning_rate':0.05,'min_child_samples':350, 'n_estimators':370,'subsample_freq':10\\\n",
    "             , 'max_bin':10,'objective':'binary', 'max_depth':4, 'num_leaves':30, 'num_threads':4,'subsample':.8, 'colsample_bytree':.8}\n",
    "\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "\n",
    "#best_xgb\n",
    "xgb_model = XGBClassifier(    \n",
    "                        n_estimators=400,\n",
    "                        max_depth=5,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=0.07, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=7,\n",
    "                        colsample_bytree=.8,\n",
    "                        scale_pos_weight=1.6,\n",
    "                        gamma=11,\n",
    "                        reg_alpha=5,\n",
    "                        reg_lambda=1.6,\n",
    "                            nthread=4\n",
    ")\n",
    "\n",
    "#best_rgf\n",
    "rgf_model = RGFClassifier(\n",
    "        max_leaf=1000,\n",
    "        algorithm=\"RGF\",  \n",
    "        loss=\"Log\",\n",
    "        l2=0.01,\n",
    "        sl2=0.01,\n",
    "        normalize=False,\n",
    "        min_samples_leaf=10,\n",
    "        n_iter=None,\n",
    "        opt_interval=100,\n",
    "        learning_rate=.5,\n",
    "        calc_prob=\"sigmoid\",\n",
    "        n_jobs=-1,\n",
    "        memory_policy=\"generous\",\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH, na_values=\"-1\") # .iloc[0:200,:]\n",
    "test_df = pd.read_csv(TEST_PATH, na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "X = train_df[train_features_bestxgb]\n",
    "test_df = test_df[train_features_bestxgb]\n",
    "\n",
    "#cat columns to target encode\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSTARTING ITERATION FOR\u001b[0;0m \u001b[1mlgb1\u001b[0;0m\n",
      "Fold 0\n",
      "Train Gini = 0.453337512928    holdout Gini = 0.254025905363\n",
      "Fold 1\n",
      "Train Gini = 0.449149416397    holdout Gini = 0.24560588904\n",
      "Fold 2\n",
      "Train Gini = 0.452085700473    holdout Gini = 0.251745059506\n",
      "Fold 3\n",
      "Train Gini = 0.451292794451    holdout Gini = 0.254442722938\n",
      "Fold 4\n",
      "Train Gini = 0.448115282752    holdout Gini = 0.29270420102\n",
      "\n",
      "Gini for full holdout set: 0.259472818537\n",
      "Avg Train gini = 0.4507961414    Avg holdout gini = 0.259704755573\n",
      "Fold Variance for train = 0.002    Fold Variance for holdouts = 0.017\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 21.49 seconds.\n"
     ]
    }
   ],
   "source": [
    "lgb_strain, lgb_stest = get_te_base_results(lgb_model, X, y, test_df, 'lgb1', f_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSTARTING ITERATION FOR\u001b[0;0m \u001b[1mte_xgb1\u001b[0;0m\n",
      "Fold 0\n",
      "Train Gini = 0.375217614083    holdout Gini = 0.278800336748\n",
      "Fold 1\n",
      "Train Gini = 0.370816902145    holdout Gini = 0.263590814104\n",
      "Fold 2\n",
      "Train Gini = 0.371373877905    holdout Gini = 0.270695293899\n",
      "Fold 3\n",
      "Train Gini = 0.37503736193    holdout Gini = 0.263621748999\n",
      "Fold 4\n",
      "Train Gini = 0.378883948847    holdout Gini = 0.250950124139\n",
      "\n",
      "Gini for full holdout set: 0.265400244922\n",
      "Avg Train gini = 0.374265940982    Avg holdout gini = 0.265531663578\n",
      "Fold Variance for train = 0.003    Fold Variance for holdouts = 0.009\n",
      "\n",
      " Time taken: 0 hours 2 minutes and 7.19 seconds.\n"
     ]
    }
   ],
   "source": [
    "xgb_strain, xgb_stest = get_te_base_results(xgb_model, X, y, test_df, 'te_xgb1', f_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSTARTING ITERATION FOR\u001b[0;0m \u001b[1mte_rgf\u001b[0;0m\n",
      "Fold 0\n",
      "Train Gini = 0.432728844686    holdout Gini = 0.253327861473\n",
      "Fold 1\n",
      "Train Gini = 0.429529108395    holdout Gini = 0.247963971695\n",
      "Fold 2\n",
      "Train Gini = 0.424535760836    holdout Gini = 0.263353880933\n",
      "Fold 3\n",
      "Train Gini = 0.434713897729    holdout Gini = 0.242758673472\n",
      "Fold 4\n",
      "Train Gini = 0.439071389702    holdout Gini = 0.242465224461\n",
      "\n",
      "Gini for full holdout set: 0.249811207804\n",
      "Avg Train gini = 0.43211580027    Avg holdout gini = 0.249973922407\n",
      "Fold Variance for train = 0.005    Fold Variance for holdouts = 0.008\n",
      "\n",
      " Time taken: 0 hours 4 minutes and 38.68 seconds.\n"
     ]
    }
   ],
   "source": [
    "rgf_strain, rgf_stest = get_te_base_results(rgf_model, X, y, test_df, 'te_rgf', f_cats, fillna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save oof and test predictons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
