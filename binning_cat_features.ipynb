{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    \"\"\"Prints time\n",
    "    \n",
    "    Initiate a time object, and prints total time consumed when again initialized object is passed as argument\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        start_time {[object]} -- initialized time object (default: {None})\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download the data from competition directory and place in porto_data directory:\n",
    "#https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data\n",
    "TRAIN_PATH = 'porto_data/train.csv'\n",
    "TEST_PATH = 'porto_data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "# __author__ = 'Victor Ruiz, vmr11@pitt.edu'\n",
    "######\n",
    "from math import log\n",
    "import random\n",
    "\n",
    "\n",
    "def entropy(data_classes, base=2):\n",
    "    '''\n",
    "    Computes the entropy of a set of labels (class instantiations)\n",
    "    :param base: logarithm base for computation\n",
    "    :param data_classes: Series with labels of examples in a dataset\n",
    "    :return: value of entropy\n",
    "    '''\n",
    "    if not isinstance(data_classes, pd.core.series.Series):\n",
    "        raise AttributeError('input array should be a pandas series')\n",
    "    classes = data_classes.unique()\n",
    "    N = len(data_classes)\n",
    "    ent = 0  # initialize entropy\n",
    "\n",
    "    # iterate over classes\n",
    "    for c in classes:\n",
    "        partition = data_classes[data_classes == c]  # data with class = c\n",
    "        proportion = len(partition) / N\n",
    "        #update entropy\n",
    "        \n",
    "        ent -= proportion * log(proportion, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "def cut_point_information_gain(dataset, cut_point, feature_label, class_label):\n",
    "    '''\n",
    "    Return de information gain obtained by splitting a numeric attribute in two according to cut_point\n",
    "    :param dataset: pandas dataframe with a column for attribute values and a column for class\n",
    "    :param cut_point: threshold at which to partition the numeric attribute\n",
    "    :param feature_label: column label of the numeric attribute values in data\n",
    "    :param class_label: column label of the array of instance classes\n",
    "    :return: information gain of partition obtained by threshold cut_point\n",
    "    '''\n",
    "    if not isinstance(dataset, pd.core.frame.DataFrame):\n",
    "        raise AttributeError('input dataset should be a pandas data frame')\n",
    "\n",
    "    entropy_full = entropy(dataset[class_label])  # compute entropy of full dataset (w/o split)\n",
    "\n",
    "    #split data at cut_point\n",
    "    data_left = dataset[dataset[feature_label] <= cut_point]\n",
    "    data_right = dataset[dataset[feature_label] > cut_point]\n",
    "    (N, N_left, N_right) = (len(dataset), len(data_left), len(data_right))\n",
    "\n",
    "    gain = entropy_full - (N_left / N) * entropy(data_left[class_label]) - \\\n",
    "        (N_right / N) * entropy(data_right[class_label])\n",
    "\n",
    "    return gain\n",
    "\n",
    "######\n",
    "# __author__ = 'Victor Ruiz, vmr11@pitt.edu'\n",
    "######\n",
    "import sys\n",
    "import getopt\n",
    "import re\n",
    "\n",
    "class MDLP_Discretizer(object):\n",
    "    def __init__(self, dataset, testset, class_label, out_path_data, out_test_path_data, out_path_bins, features=None):\n",
    "        '''\n",
    "        initializes discretizer object:\n",
    "            saves raw copy of data and creates self._data with only features to discretize and class\n",
    "            computes initial entropy (before any splitting)\n",
    "            self._features = features to be discretized\n",
    "            self._classes = unique classes in raw_data\n",
    "            self._class_name = label of class in pandas dataframe\n",
    "            self._data = partition of data with only features of interest and class\n",
    "            self._cuts = dictionary with cut points for each feature\n",
    "        :param dataset: pandas dataframe with data to discretize\n",
    "        :param class_label: name of the column containing class in input dataframe\n",
    "        :param features: if !None, features that the user wants to discretize specifically\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        if not isinstance(dataset, pd.core.frame.DataFrame):  # class needs a pandas dataframe\n",
    "            raise AttributeError('Input dataset should be a pandas data frame')\n",
    "\n",
    "        if not isinstance(testset, pd.core.frame.DataFrame):  # class needs a pandas dataframe\n",
    "            raise AttributeError('Test dataset should be a pandas data frame')\n",
    "\n",
    "        self._data_raw = dataset #copy or original input data\n",
    "        self._test_raw = testset #copy or original test data\n",
    "\n",
    "        self._class_name = class_label\n",
    "\n",
    "        self._classes = self._data_raw[self._class_name].unique()\n",
    "\n",
    "        #if user specifies which attributes to discretize\n",
    "        if features:\n",
    "            self._features = [f for f in features if f in self._data_raw.columns]  # check if features in dataframe\n",
    "            missing = set(features) - set(self._features)  # specified columns not in dataframe\n",
    "            if missing:\n",
    "                print ('WARNING: user-specified features %s not in input dataframe' % str(missing))\n",
    "        else:  # then we need to recognize which features are numeric\n",
    "            numeric_cols = self._data_raw._data.get_numeric_data().items\n",
    "            self._features = [f for f in numeric_cols if f != class_label]\n",
    "        #other features that won't be discretized\n",
    "        self._ignored_features = set(self._data_raw.columns) - set(self._features)\n",
    "        self._ignored_features_t = set(self._test_raw.columns) - set(self._features)\n",
    "\n",
    "        #create copy of data only including features to discretize and class\n",
    "        self._data = self._data_raw.loc[:, self._features + [class_label]]\n",
    "        self._test = self._test_raw.loc[:, self._features]\n",
    "        #pre-compute all boundary points in dataset\n",
    "        self._boundaries = self.compute_boundary_points_all_features()\n",
    "        #initialize feature bins with empty arrays\n",
    "        self._cuts = {f: [] for f in self._features}\n",
    "        #get cuts for all features\n",
    "        self.all_features_accepted_cutpoints()\n",
    "        #discretize self._data\n",
    "        self.apply_cutpoints(out_data_path=out_path_data, out_test_path=out_test_path_data, out_bins_path=out_path_bins)\n",
    "\n",
    "    def MDLPC_criterion(self, data, feature, cut_point):\n",
    "        '''\n",
    "        Determines whether a partition is accepted according to the MDLPC criterion\n",
    "        :param feature: feature of interest\n",
    "        :param cut_point: proposed cut_point\n",
    "        :param partition_index: index of the sample (dataframe partition) in the interval of interest\n",
    "        :return: True/False, whether to accept the partition\n",
    "        '''\n",
    "        #get dataframe only with desired attribute and class columns, and split by cut_point\n",
    "        data_partition = data.copy(deep=True)\n",
    "        data_left = data_partition[data_partition[feature] <= cut_point]\n",
    "        data_right = data_partition[data_partition[feature] > cut_point]\n",
    "\n",
    "        #compute information gain obtained when splitting data at cut_point\n",
    "        cut_point_gain = cut_point_information_gain(dataset=data_partition, cut_point=cut_point,\n",
    "                                                    feature_label=feature, class_label=self._class_name)\n",
    "        #compute delta term in MDLPC criterion\n",
    "        N = len(data_partition) # number of examples in current partition\n",
    "        partition_entropy = entropy(data_partition[self._class_name])\n",
    "        k = len(data_partition[self._class_name].unique())\n",
    "        k_left = len(data_left[self._class_name].unique())\n",
    "        k_right = len(data_right[self._class_name].unique())\n",
    "        entropy_left = entropy(data_left[self._class_name])  # entropy of partition\n",
    "        entropy_right = entropy(data_right[self._class_name])\n",
    "        delta = log(3 ** k, 2) - (k * partition_entropy) + (k_left * entropy_left) + (k_right * entropy_right)\n",
    "\n",
    "        #to split or not to split\n",
    "        gain_threshold = (log(N - 1, 2) + delta) / N\n",
    "\n",
    "        if cut_point_gain > gain_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def feature_boundary_points(self, data, feature):\n",
    "        '''\n",
    "        Given an attribute, find all potential cut_points (boundary points)\n",
    "        :param feature: feature of interest\n",
    "        :param partition_index: indices of rows for which feature value falls whithin interval of interest\n",
    "        :return: array with potential cut_points\n",
    "        '''\n",
    "        #get dataframe with only rows of interest, and feature and class columns\n",
    "        data_partition = data.copy(deep=True)\n",
    "        data_partition.sort_values(feature, ascending=True, inplace=True)\n",
    "\n",
    "        boundary_points = []\n",
    "\n",
    "        #add temporary columns\n",
    "        data_partition['class_offset'] = data_partition[self._class_name].shift(1)  # column where first value is now second, and so forth\n",
    "        data_partition['feature_offset'] = data_partition[feature].shift(1)  # column where first value is now second, and so forth\n",
    "        data_partition['feature_change'] = (data_partition[feature] != data_partition['feature_offset'])\n",
    "        data_partition['mid_points'] = data_partition.loc[:, [feature, 'feature_offset']].mean(axis=1)\n",
    "\n",
    "        potential_cuts = data_partition[data_partition['feature_change'] == True].index[1:]\n",
    "        sorted_index = data_partition.index.tolist()\n",
    "\n",
    "        for row in potential_cuts:\n",
    "            old_value = data_partition.loc[sorted_index[sorted_index.index(row) - 1]][feature]\n",
    "            new_value = data_partition.loc[row][feature]\n",
    "            old_classes = data_partition[data_partition[feature] == old_value][self._class_name].unique()\n",
    "            new_classes = data_partition[data_partition[feature] == new_value][self._class_name].unique()\n",
    "            if len(set.union(set(old_classes), set(new_classes))) > 1:\n",
    "                boundary_points += [data_partition.loc[row]['mid_points']]\n",
    "\n",
    "        return set(boundary_points)\n",
    "\n",
    "    def compute_boundary_points_all_features(self):\n",
    "        '''\n",
    "        Computes all possible boundary points for each attribute in self._features (features to discretize)\n",
    "        :return:\n",
    "        '''\n",
    "        boundaries = {}\n",
    "        for attr in self._features:\n",
    "            data_partition = self._data.loc[:, [attr, self._class_name]]\n",
    "            boundaries[attr] = self.feature_boundary_points(data=data_partition, feature=attr)\n",
    "        return boundaries\n",
    "\n",
    "    def boundaries_in_partition(self, data, feature):\n",
    "        '''\n",
    "        From the collection of all cut points for all features, find cut points that fall within a feature-partition's\n",
    "        attribute-values' range\n",
    "        :param data: data partition (pandas dataframe)\n",
    "        :param feature: attribute of interest\n",
    "        :return: points within feature's range\n",
    "        '''\n",
    "        range_min, range_max = (data[feature].min(), data[feature].max())\n",
    "        return set([x for x in self._boundaries[feature] if (x > range_min) and (x < range_max)])\n",
    "\n",
    "    def best_cut_point(self, data, feature):\n",
    "        '''\n",
    "        Selects the best cut point for a feature in a data partition based on information gain\n",
    "        :param data: data partition (pandas dataframe)\n",
    "        :param feature: target attribute\n",
    "        :return: value of cut point with highest information gain (if many, picks first). None if no candidates\n",
    "        '''\n",
    "        candidates = self.boundaries_in_partition(data=data, feature=feature)\n",
    "        # candidates = self.feature_boundary_points(data=data, feature=feature)\n",
    "        if not candidates:\n",
    "            return None\n",
    "        gains = [(cut, cut_point_information_gain(dataset=data, cut_point=cut, feature_label=feature,\n",
    "                                                  class_label=self._class_name)) for cut in candidates]\n",
    "        gains = sorted(gains, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return gains[0][0] #return cut point\n",
    "\n",
    "    def single_feature_accepted_cutpoints(self, feature, partition_index=pd.DataFrame().index):\n",
    "        '''\n",
    "        Computes the cuts for binning a feature according to the MDLP criterion\n",
    "        :param feature: attribute of interest\n",
    "        :param partition_index: index of examples in data partition for which cuts are required\n",
    "        :return: list of cuts for binning feature in partition covered by partition_index\n",
    "        '''\n",
    "        if partition_index.size == 0:\n",
    "            partition_index = self._data.index  # if not specified, full sample to be considered for partition\n",
    "\n",
    "        data_partition = self._data.loc[partition_index, [feature, self._class_name]]\n",
    "\n",
    "        #exclude missing data:\n",
    "        if data_partition[feature].isnull().values.any:\n",
    "            data_partition = data_partition[~data_partition[feature].isnull()]\n",
    "\n",
    "        #stop if constant or null feature values\n",
    "        if len(data_partition[feature].unique()) < 2:\n",
    "            return\n",
    "        #determine whether to cut and where\n",
    "        cut_candidate = self.best_cut_point(data=data_partition, feature=feature)\n",
    "        if cut_candidate == None:\n",
    "            return\n",
    "        decision = self.MDLPC_criterion(data=data_partition, feature=feature, cut_point=cut_candidate)\n",
    "\n",
    "        #apply decision\n",
    "        if not decision:\n",
    "            return  # if partition wasn't accepted, there's nothing else to do\n",
    "        if decision:\n",
    "            # try:\n",
    "            #now we have two new partitions that need to be examined\n",
    "            left_partition = data_partition[data_partition[feature] <= cut_candidate]\n",
    "            right_partition = data_partition[data_partition[feature] > cut_candidate]\n",
    "            if left_partition.empty or right_partition.empty:\n",
    "                return #extreme point selected, don't partition\n",
    "            self._cuts[feature] += [cut_candidate]  # accept partition\n",
    "            self.single_feature_accepted_cutpoints(feature=feature, partition_index=left_partition.index)\n",
    "            self.single_feature_accepted_cutpoints(feature=feature, partition_index=right_partition.index)\n",
    "            #order cutpoints in ascending order\n",
    "            self._cuts[feature] = sorted(self._cuts[feature])\n",
    "            return\n",
    "\n",
    "    def all_features_accepted_cutpoints(self):\n",
    "        '''\n",
    "        Computes cut points for all numeric features (the ones in self._features)\n",
    "        :return:\n",
    "        '''\n",
    "        for attr in self._features:\n",
    "            self.single_feature_accepted_cutpoints(feature=attr)\n",
    "        return\n",
    "\n",
    "    def apply_cutpoints(self, out_data_path=None, out_test_path=None, out_bins_path=None):\n",
    "        '''\n",
    "        Discretizes data by applying bins according to self._cuts. Saves a new, discretized file, and a description of\n",
    "        the bins\n",
    "        :param out_data_path: path to save discretized data\n",
    "        :param out_test_path: path to save discretized test data\n",
    "        :param out_bins_path: path to save bins description\n",
    "        :return:\n",
    "        '''\n",
    "        pbin_label_collection = {}\n",
    "        bin_label_collection = {}\n",
    "        for attr in self._features:\n",
    "            if len(self._cuts[attr]) == 0:\n",
    "#                self._data[attr] = 'All'\n",
    "                self._data[attr] = self._data[attr].values\n",
    "                self._test[attr] = self._test[attr].values\n",
    "                pbin_label_collection[attr] = ['No binning']\n",
    "                bin_label_collection[attr] = ['All']\n",
    "            else:\n",
    "                cuts = [-np.inf] + self._cuts[attr] + [np.inf]\n",
    "                print(attr, cuts)\n",
    "                start_bin_indices = range(0, len(cuts) - 1)\n",
    "                pbin_labels = ['%s_to_%s' % (str(cuts[i]), str(cuts[i+1])) for i in start_bin_indices]\n",
    "                bin_labels = ['%d' % (i+1) for i in start_bin_indices]\n",
    "                pbin_label_collection[attr] = pbin_labels\n",
    "                bin_label_collection[attr] = bin_labels\n",
    "                self._data[attr] = pd.cut(x=self._data[attr].values, bins=cuts, right=False, labels=bin_labels,\n",
    "                                          precision=6, include_lowest=True)\n",
    "                self._test[attr] = pd.cut(x=self._test[attr].values, bins=cuts, right=False, labels=bin_labels,\n",
    "                                          precision=6, include_lowest=True)\n",
    "\n",
    "        #reconstitute full data, now discretized\n",
    "        if self._ignored_features:\n",
    "        #the line below may help in removing double class column ; looks like it works\n",
    "            self._data = self._data.loc[:, self._features]\n",
    "            to_return_train = pd.concat([self._data, self._data_raw[list(self._ignored_features)]], axis=1)\n",
    "            to_return_train = to_return_train[self._data_raw.columns] #sort columns so they have the original order\n",
    "        else:\n",
    "        #the line below may help in removing double class column ; looks like it works\n",
    "            self._data = self._data.loc[:, self._features]\n",
    "            to_return_train = self._data\n",
    "\n",
    "        #save data as csv\n",
    "        if out_data_path:\n",
    "            to_return_train.to_csv(out_data_path, index=False)\n",
    "\n",
    "        #reconstitute test data, now discretized\n",
    "        if self._ignored_features:\n",
    "        #the line below may help in removing double class column ; looks like it works\n",
    "        #    self._test = self._test.loc[:, self._features]\n",
    "            to_return_test = pd.concat([self._test, self._test_raw[list(self._ignored_features_t)]], axis=1)\n",
    "            to_return_test = to_return_test[self._test_raw.columns] #sort columns so they have the original order\n",
    "        else:\n",
    "        #the line below may help in removing double class column ; looks like it works\n",
    "        #    self._data = self._data.loc[:, self._features]\n",
    "            to_return_test = self._test\n",
    "\n",
    "        #save data as csv\n",
    "        if out_test_path:\n",
    "            to_return_test.to_csv(out_test_path, index=False)\n",
    "\n",
    "        #save bins description\n",
    "        if out_bins_path:\n",
    "            with open(out_bins_path, 'w') as bins_file:\n",
    "                print>>bins_file, 'Description of bins in file: %s' % out_data_path\n",
    "                for attr in self._features:\n",
    "                    print>>bins_file, 'attr: %s\\n\\t%s' % (attr, ', '.join([pbin_label for pbin_label in pbin_label_collection[attr]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train dataset: (119043, 59)\n",
      " Test dataset: (250, 58)\n",
      "ps_reg_03 [-inf, 0.67984369320000004, inf]\n",
      "ps_car_12 [-inf, 0.37262581165000003, 0.54593953984999999, inf]\n",
      "ps_car_13 [-inf, 0.63278212904999998, 0.83260643204999996, 1.2181084610999999, inf]\n",
      "ps_car_14 [-inf, 0.34358404310000001, 0.41623299945000003, inf]\n",
      "\n",
      " Time taken: 0 hours 7 minutes and 18.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH, dtype={'id': np.int32, 'target': np.int8})\n",
    "print(' Train dataset:', df_train.shape)\n",
    "train = df_train.drop(['id', 'target'], axis=1)\n",
    "target = df_train['target']\n",
    "df_train = df_train.replace(-1, np.nan)\n",
    "df_test = pd.read_csv(TEST_PATH, dtype={'id': np.int32})\n",
    "print(' Test dataset:', df_test.shape)\n",
    "test = df_test.drop(['id'], axis=1)\n",
    "df_test = df_test.replace(-1, np.nan)\n",
    "\n",
    "class_label='target' \n",
    "#below features are already binned\n",
    "features=['ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_14']\n",
    "# features=[]\n",
    "\n",
    "start_time=timer(None)\n",
    "discretizer = MDLP_Discretizer(dataset=df_train, testset=df_test, class_label=class_label, features=features, out_path_data='./porto_data/bin_train.csv', out_test_path_data='./porto_data/bin_test.csv', out_path_bins=None)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
